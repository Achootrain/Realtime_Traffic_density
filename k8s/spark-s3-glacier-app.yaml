apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-s3-glacier
  namespace: traffic
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: ghcr.io/achootrain/spark-application:latest
  imagePullPolicy: Always 
  imagePullSecrets:
    - ghcr-secret
  mainApplicationFile: local:///app/s3_glacier.py
  sparkVersion: 3.5.1
  sparkConf:
    "spark.jars.ivy": "/tmp/.ivy2"
    "spark.jars": "local:///opt/spark/jars/hadoop-aws-3.3.4.jar,local:///opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar,local:///opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,local:///opt/spark/jars/kafka-clients-3.5.1.jar,local:///opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,local:///opt/spark/jars/commons-pool2-2.11.1.jar"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    # Ensure env var is present even if the operator doesn't propagate spec.driver.env
    "spark.kubernetes.driverEnv.S3_BUCKET": "traffic-count-bigdata"
    "spark.kubernetes.executorEnv.S3_BUCKET": "traffic-count-bigdata"
  driver:
    coreRequest: "100m"
    coreLimit: "300m"
    memory: 512m
    serviceAccount: default
    labels:
      version: "3.5.1"
    secrets:
      - name: aws-credentials
        path: /etc/secrets/aws-credentials
        secretType: Generic
    env:
      - name: HOME
        value: "/tmp"
      - name: USER
        value: "spark"
      - name: KAFKA_BOOTSTRAP_SERVERS
        value: "kafka.traffic.svc.cluster.local:9092"
      - name: KAFKA_TOPIC
        value: "hugedata"
      - name: KAFKA_GROUP_ID
        value: "spark-s3-glacier-group"
      - name: AWS_REGION
        value: "us-east-1"
      - name: S3_BUCKET
        value: "traffic-count-bigdata"
      - name: S3_STORAGE_CLASS
        value: "STANDARD"
    securityContext:
      runAsUser: 185
      runAsGroup: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      seccompProfile:
        type: RuntimeDefault
  executor:
    instances: 1
    coreRequest: "100m"
    coreLimit: "300m"
    memory: 512m
    labels:
      version: "3.5.1"
    secrets:
      - name: aws-credentials
        path: /etc/secrets/aws-credentials
        secretType: Generic
    env:
      - name: HOME
        value: "/tmp"
      - name: USER
        value: "spark"
      - name: KAFKA_BOOTSTRAP_SERVERS
        value: "kafka.traffic.svc.cluster.local:9092"
      - name: KAFKA_TOPIC
        value: "hugedata"
      - name: KAFKA_GROUP_ID
        value: "spark-s3-glacier-group"
      - name: AWS_REGION
        value: "us-east-1"
      - name: S3_BUCKET
        value: "traffic-count-bigdata"
      - name: S3_STORAGE_CLASS
        value: "STANDARD"
    securityContext:
      runAsUser: 185
      runAsGroup: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      seccompProfile:
        type: RuntimeDefault