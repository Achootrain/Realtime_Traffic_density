name: Build & Deploy to GHCR (K3s)

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      service:
        description: "Which service to deploy?"
        type: choice
        required: true
        default: kafka
        options:
          - kafka
          - spark

permissions:
  contents: read
  packages: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # 1. Chuẩn hóa tên Repo (Chữ thường)
      - name: Lowercase Repo Owner
        run: |
          echo "REPO_OWNER=${GITHUB_REPOSITORY_OWNER,,}" >> ${GITHUB_ENV}

      # 2. Logic chọn service
      - name: Resolve build target
        id: target
        shell: bash
        run: |
          set -e
          CHOSEN_SERVICE="${{ github.event.inputs.service }}"
          
          if [[ -z "$CHOSEN_SERVICE" ]]; then
            if git diff --name-only HEAD^ HEAD | grep -q "^spark/"; then
               CHOSEN_SERVICE="spark"
            elif git diff --name-only HEAD^ HEAD | grep -q "^kafka/"; then
               CHOSEN_SERVICE="kafka"
            else
               CHOSEN_SERVICE="kafka"
            fi
          fi
          
          echo "service=$CHOSEN_SERVICE" >> "$GITHUB_OUTPUT"
          
          if [[ "$CHOSEN_SERVICE" == "kafka" ]]; then
             echo "context=kafka" >> "$GITHUB_OUTPUT"
             echo "dockerfile=kafka/dockerfile" >> "$GITHUB_OUTPUT"
             echo "image_name=traffic-kafka" >> "$GITHUB_OUTPUT"
          elif [[ "$CHOSEN_SERVICE" == "spark" ]]; then
             echo "context=spark" >> "$GITHUB_OUTPUT"
             echo "dockerfile=spark/dockerfile" >> "$GITHUB_OUTPUT"
             echo "image_name=traffic-spark" >> "$GITHUB_OUTPUT"
          fi

      # 3-4. Skip build & push: use public Docker Hub images (achootrain/*)

      # 5. Copy file YAML và init.sql lên Server
      - name: Copy k8s manifests to EC2
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          source: "k8s/*.yaml,timescaledb/init.sql,clean.sh"
          target: "/home/ubuntu/"

      # 6. Deploy lên K3s (Phần quan trọng nhất)
      - name: Deploy on K3s
        uses: appleboy/ssh-action@v1.0.3
        env:
          REPO_OWNER: ${{ env.REPO_OWNER }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          SERVICE_NAME: ${{ steps.target.outputs.service }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          GHCR_USERNAME: ${{ secrets.GHCR_USERNAME }}
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          envs: REPO_OWNER,EC2_HOST,SERVICE_NAME,AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,GHCR_USERNAME,GHCR_TOKEN
          script: |
            set -e
            
            # A. CẤP QUYỀN KUBECTL (Để user ubuntu chạy được lệnh)
            # Copy config của K3s ra chỗ user ubuntu đọc được
            mkdir -p ~/.kube
            sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
            sudo chown ubuntu:ubuntu ~/.kube/config
            sudo chmod 600 ~/.kube/config
            export KUBECONFIG=~/.kube/config
            
            cd /home/ubuntu/k8s
            
            # B. XỬ LÝ IP ĐỘNG CHO KAFKA (Quan trọng)
            # Tìm chữ 'IP_PLACEHOLDER' trong file kafka.yaml và thay bằng IP thật
            sed -i "s/IP_PLACEHOLDER/$EC2_HOST/g" kafka.yaml || true
            
            # C. Không cần thay thế username Docker Hub (dùng image cố định achootrain/*)

            echo "Setting up Helm and Spark Operator..."
            
            # Cài đặt Helm nếu chưa có
            if ! command -v helm &> /dev/null; then
              curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
            fi
            
            # Thêm Spark Operator Helm repo
            helm repo add spark-operator https://kubeflow.github.io/spark-operator || true
            helm repo update
            
            echo "Applying Kustomize manifests..."

            # Tạo namespace nếu chưa có
            kubectl create namespace traffic --dry-run=client -o yaml | kubectl apply -f -

            # D. TẠO GHCR IMAGEPULLSECRET TỪ GITHUB SECRETS (cho imagePullSecrets: ghcr-secret)
            echo "Creating GHCR imagePullSecret (ghcr-secret)..."
            kubectl create secret docker-registry ghcr-secret \
              --namespace traffic \
              --docker-server=ghcr.io \
              --docker-username="$GHCR_USERNAME" \
              --docker-password="$GHCR_TOKEN" \
              --docker-email="noreply@example.com" \
              --dry-run=client -o yaml | kubectl apply -f -
            
            # D. TẠO AWS CREDENTIALS SECRET TỪ GITHUB SECRETS
            echo "Creating AWS credentials secret..."
            kubectl create secret generic aws-credentials \
              --namespace traffic \
              --from-literal=access_key_id="$AWS_ACCESS_KEY_ID" \
              --from-literal=secret_access_key="$AWS_SECRET_ACCESS_KEY" \
              --dry-run=client -o yaml | kubectl apply -f -
            
            # Cài đặt/Upgrade Spark Operator
            helm upgrade --install spark-operator-1 spark-operator/spark-operator \
              --namespace traffic \
              --set sparkJobNamespace=traffic \
              --set watchNamespace=traffic \
              --set webhook.enable=true

            # Apply tất cả qua kustomization để thống nhất namespace & images
            kubectl apply -k .
            
            # E. INIT TIMESCALEDB (Chạy init.sql nếu chưa có table)
            echo "Initializing TimescaleDB..."
            # Wait for TimescaleDB to be ready
            kubectl wait --for=condition=ready pod/timescaledb-0 -n traffic --timeout=120s || true
            
            # Copy và chạy init.sql
            if [ -f "/home/ubuntu/timescaledb/init.sql" ]; then
              kubectl cp /home/ubuntu/timescaledb/init.sql traffic/timescaledb-0:/tmp/init.sql
              kubectl exec timescaledb-0 -n traffic -- psql -U postgres -d traffic -f /tmp/init.sql || true
              echo "TimescaleDB initialized"
            fi
            
            # F. FORCE UPDATE (Bắt buộc để code mới chạy)
            echo "Restarting deployments to pick up new image..."
            if [ "$SERVICE_NAME" == "kafka" ]; then
               # Restart Kafka StatefulSet
               kubectl rollout restart statefulset/kafka -n traffic || true
            elif [ "$SERVICE_NAME" == "spark" ]; then
               # Delete và tạo lại SparkApplication để operator deploy lại
               kubectl delete sparkapplication spark-realtime-python -n traffic --ignore-not-found
               sleep 5
               kubectl apply -f spark-realtime-app.yaml -n traffic
               echo "Spark application restarted"
            fi
            
            echo "Done."